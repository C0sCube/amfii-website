{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time\n",
    "import pandas as pd\n",
    "import pprint\n",
    " \n",
    " \n",
    "import pandas as pd\n",
    "csv_path = r\"data_code.xlsx\"\n",
    " \n",
    "df = pd.read_excel(csv_path)\n",
    " \n",
    "codes = df[\"BSE Exchange Code\"].to_list()\n",
    "print(codes[:10])\n",
    " \n",
    "url = \"https://api.bseindia.com/BseIndiaAPI/api/AnnSubCategoryGetData/w\"\n",
    " \n",
    "params_template = {\n",
    "    \"pageno\": 1,\n",
    "    \"strCat\": \"Result\",\n",
    "    \"strPrevDate\": \"20250922\",\n",
    "    \"strSearch\": \"P\",\n",
    "    \"strToDate\": \"20251222\",\n",
    "    \"strType\": \"C\",\n",
    "    \"subcategory\": -1\n",
    "}\n",
    " \n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://www.bseindia.com/\",\n",
    "    \"Origin\": \"https://www.bseindia.com\",\n",
    "    \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "}\n",
    " \n",
    "session = requests.Session()\n",
    "all_dfs = []   # collect DataFrames here\n",
    " \n",
    "for idx,code in enumerate(codes[2100:]):\n",
    "    params = params_template.copy()\n",
    "    params[\"strScrip\"] = str(code)\n",
    " \n",
    "    response = session.get(url, params=params, headers=headers, verify=False)\n",
    "    print(\"Scrip\", code, \"-> Status:\", response.status_code)\n",
    " \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"Table\" in data and data[\"Table\"]:\n",
    "            df = pd.DataFrame(data[\"Table\"])\n",
    "            # Fix attachment URL\n",
    "            df[\"ATTACHMENTNAME\"] = df[\"ATTACHMENTNAME\"].apply(\n",
    "                lambda x: \"https://www.bseindia.com/xml-data/corpfiling/AttachHis/\" + str(x)\n",
    "            )\n",
    "            # pprint.pprint(df.head(1).to_dict())  # peek at first row\n",
    "            all_dfs.append(df)\n",
    "    else:\n",
    "        print(\"‚ùå Request failed for\", code)\n",
    "   \n",
    "    time.sleep(2.5)\n",
    " \n",
    "# Concatenate all results into one DataFrame\n",
    "if all_dfs:\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    final_df.to_csv(\"all_announcements_6.csv\", index=False)\n",
    "    print(\"‚úÖ Saved\", len(final_df), \"rows to all_announcements.csv\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249347de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "download_dir = r\"C:\\Users\\rando\\Downloads\" \n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "driver.get(\"https://vahan.parivahan.gov.in/vahan4dashboard/vahan/vahan/view/reportview.xhtml\")\n",
    "\n",
    "# --- CSS selectors for dropdown items ---\n",
    "state_items_css  = \"#j_idt43_items li\"\n",
    "rto_items_css    = \"#selectedRto_items li\"\n",
    "yaxis_items_css  = \"#yaxisVar_items li\"\n",
    "xaxis_items_css  = \"#xaxisVar_items li\"\n",
    "year_items_css   = \"#selectedYear_items li\"\n",
    "\n",
    "# --- Helper for safe clicks ---\n",
    "def safe_click(locator, retries=3, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            el = wait.until(EC.element_to_be_clickable(locator))\n",
    "            driver.execute_script(\"arguments[0].click();\", el)\n",
    "            return\n",
    "        except StaleElementReferenceException:\n",
    "            print(\"Stale element, retrying:\", locator)\n",
    "            time.sleep(delay)\n",
    "    raise Exception(f\"Could not click {locator} after {retries} retries\")\n",
    "\n",
    "\n",
    "# --- Select Month ---\n",
    "def select_month(month_label, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Always re-open the dropdown to get a fresh DOM\n",
    "            safe_click((By.ID, \"groupingTable:selectMonth_label\"))\n",
    "            time.sleep(0.4)\n",
    "\n",
    "            # Re-locate the overlay items each time\n",
    "            month_items = wait.until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#groupingTable\\\\:selectMonth_items li\"))\n",
    "            )\n",
    "\n",
    "            # Find the desired month by text\n",
    "            for m in month_items:\n",
    "                if m.text.strip() == month_label:\n",
    "                    driver.execute_script(\"arguments[0].click();\", m)\n",
    "                    return\n",
    "        except StaleElementReferenceException:\n",
    "            print(f\"Stale element while selecting {month_label}, retrying...\")\n",
    "            time.sleep(0.5)\n",
    "    raise Exception(f\"Could not select month {month_label} after {retries} retries\")\n",
    "\n",
    "\n",
    "all_dataframes = []\n",
    "\n",
    "# --- Open States dropdown once to get list ---\n",
    "safe_click((By.ID, \"j_idt43_label\")) #\"j_idt38_label\"\n",
    "time.sleep(0.5)\n",
    "states = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, state_items_css)))\n",
    "\n",
    "for i in range(30, 32):  #len(states) # skip \"All States\" #do haryana\n",
    "    safe_click((By.ID, \"j_idt43_label\"))\n",
    "    time.sleep(0.4)\n",
    "    states = driver.find_elements(By.CSS_SELECTOR, state_items_css)\n",
    "\n",
    "    state = states[i]\n",
    "    print(\"\\nSTATE:\", state.text)\n",
    "    driver.execute_script(\"arguments[0].click();\", state)\n",
    "    time.sleep(.5)\n",
    "\n",
    "    # --- Wait for RTOs ---\n",
    "    wait.until(lambda d: len(d.find_elements(By.CSS_SELECTOR, rto_items_css)) > 1)\n",
    "    safe_click((By.ID, \"selectedRto_label\"))\n",
    "    time.sleep(0.4)\n",
    "    rtos = driver.find_elements(By.CSS_SELECTOR, rto_items_css)\n",
    "\n",
    "    for r in rtos[1:]:   # skip \"All Offices\"\n",
    "        print(\"RTO:\", r.text)\n",
    "        driver.execute_script(\"arguments[0].click();\", r)\n",
    "        time.sleep(.3)\n",
    "\n",
    "        # --- Set Y-Axis ---\n",
    "        safe_click((By.ID, \"yaxisVar_label\"))\n",
    "        time.sleep(0.3)\n",
    "        for opt in driver.find_elements(By.CSS_SELECTOR, yaxis_items_css):\n",
    "            if \"Vehicle Category\" in opt.text:\n",
    "                driver.execute_script(\"arguments[0].click();\", opt)\n",
    "                break\n",
    "\n",
    "        # --- Set X-Axis = Month Wise ---\n",
    "        safe_click((By.ID, \"xaxisVar_label\"))   # open dropdown\n",
    "        time.sleep(0.4)\n",
    "\n",
    "        xaxis_opts = driver.find_elements(By.CSS_SELECTOR, \"#xaxisVar_items li\")\n",
    "        for opt in xaxis_opts:\n",
    "            if \"Vehicle Class\" in opt.text or \"Vehicle Class\" in opt.text:\n",
    "                driver.execute_script(\"arguments[0].click();\", opt)\n",
    "                break\n",
    "\n",
    "        # --- Set Year ---\n",
    "        safe_click((By.ID, \"selectedYear_label\"))\n",
    "        time.sleep(0.4)\n",
    "        for opt in driver.find_elements(By.CSS_SELECTOR, year_items_css):\n",
    "            if \"2025\" in opt.text:\n",
    "                driver.execute_script(\"arguments[0].click();\", opt)\n",
    "                break\n",
    "\n",
    "        # --- Click Refresh ---\n",
    "        safe_click((By.ID, \"j_idt85\"))\n",
    "        time.sleep(.5)\n",
    "        print(\"\\tRefreshed for\", state.text, \"->\", r.text)\n",
    "        \n",
    "        \n",
    "        # --- Select Month ---\n",
    "\n",
    "        months = [\"JAN\",\"FEB\",\"MAR\",\"APR\",\"MAY\",\"JUN\",\"JUL\",\"AUG\",\"SEP\",\"OCT\",\"NOV\",\"DEC\"]\n",
    "        for month in months:\n",
    "            select_month(month)\n",
    "            time.sleep(0.3)\n",
    "            # --- Click Excel download ---\n",
    "            safe_click((By.ID, \"groupingTable:xls\"))\n",
    "            time.sleep(.6)\n",
    "\n",
    "            # files = sorted([os.path.join(download_dir, f) for f in os.listdir(download_dir)], key=os.path.getmtime)\n",
    "            # path = files[-1]\n",
    "\n",
    "            # # --- Detect format ---\n",
    "            # ext = os.path.splitext(path)[1].lower()\n",
    "\n",
    "            # try:\n",
    "            #     if ext in [\".xls\", \".xlsx\"]:\n",
    "            #         # Try Excel first\n",
    "            #         try:\n",
    "            #             df = pd.read_excel(path, engine=\"openpyxl\")  # for .xlsx\n",
    "            #         except Exception:\n",
    "            #             # Fallback: treat as CSV if Excel parsing fails\n",
    "            #             df = pd.read_csv(path)\n",
    "            #     else:\n",
    "            #         # Default to CSV\n",
    "            #         df = pd.read_csv(path)\n",
    "\n",
    "            #     print(\"Downloaded shape:\", df.shape)\n",
    "\n",
    "\n",
    "                # --- Clean content ---\n",
    "                # content = df.iloc[2:, 1:]\n",
    "                # category_class = df.columns[0]\n",
    "                # rto_clean = re.sub(r\"\\s+\", \"\", category_class, flags=re.IGNORECASE)\n",
    "                # content[\"RTO\"] = [rto_clean] * content.shape[0]\n",
    "                # content[\"STATE\"] = state.text\n",
    "\n",
    "                # # --- Check empty/non-empty ---\n",
    "                # if df.shape[1] > 8 and df.shape[0]>3:\n",
    "                #     print(\"‚úÖ Non-empty table for\", state.text, \"->\", r.text)\n",
    "                # else:\n",
    "                #     print(\"‚ö†Ô∏è Empty table for\", state.text, \"->\", r.text)\n",
    "\n",
    "                # all_dataframes.append(content)\n",
    "            \n",
    "            # except Exception as e:\n",
    "            #     print(\"‚ùå Failed to parse downloaded file:\", e)\n",
    "\n",
    "# if all_dataframes:\n",
    "#     master_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "#     master_df.to_excel(\"vahan_master_class.xlsx\", index=False)\n",
    "#     print(\"Saved consolidated data to vahan_master_class.xlsx\")\n",
    "# else:\n",
    "#     print(\"No data collected.\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab07737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = r\"C:\\Users\\kaustubh.keny\\Downloads\\kk\"\n",
    "\n",
    "for i, file in enumerate(os.listdir(folder_path), start=1):\n",
    "    if file.lower().endswith(\".xlsx\"):\n",
    "        old_path = os.path.join(folder_path, file)\n",
    "        new_name = f\"report_j{i}.xlsx\"   # customize pattern here\n",
    "        new_path = os.path.join(folder_path, new_name)\n",
    "        \n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"Renamed: {file} -> {new_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b96abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = r\"C:\\Users\\rando\\Downloads\\data-2023\"\n",
    "\n",
    "# collect all dataframes here\n",
    "global_data = []\n",
    "\n",
    "regex = r\"Vehicle Category Wise Vehicle Class Data\\s*of(.*),\\s+((?:Andaman & Nicobar Islands|Andhra Pradesh|Arunachal Pradesh|Assam|Bihar|Chhattisgarh|Chandigarh|UT of DNH and DD|Delhi|Goa|Gujarat|Himachal Pradesh|Haryana|Jharkhand|Jammu and Kashmir|Karnataka|Kerala|Ladakh|Lakshadweep|Maharashtra|Meghalaya|Manipur|Madhya Pradesh|Mizoram|Nagaland|Odisha|Punjab|Puducherry|Rajasthan|Sikkim|Tamil Nadu|Tripura|Uttarakhand|Uttar Pradesh|West Bengal))\\s+\\(([A-Z]+,\\d{4})\\)$\"\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    try:\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        if ext in [\".xls\", \".xlsx\"]:\n",
    "            try:\n",
    "                df = pd.read_excel(file_path, engine=\"openpyxl\", header=None)\n",
    "            except Exception:\n",
    "                df = pd.read_csv(file_path, header=None)\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "        name = str(df.iloc[0, 0])\n",
    "        content = df.iloc[2:, 1:]  # skip header rows, drop first col\n",
    "        row, col = content.shape\n",
    "\n",
    "        if matches := re.findall(regex, name, re.IGNORECASE):\n",
    "            rto, state, date = matches[0]\n",
    "            add_data = pd.DataFrame({\n",
    "                \"RTO\":   [rto.strip()]   * row,\n",
    "                \"STATE\": [state.strip()] * row,\n",
    "                \"DATE\":  [date.strip()]  * row\n",
    "            })\n",
    "            final_df = pd.concat([add_data, content.reset_index(drop=True)], axis=1)\n",
    "            global_data.append(final_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Failed to parse downloaded file:\", file, e)\n",
    "\n",
    "# combine all into one master dataframe\n",
    "if global_data:\n",
    "    master_df = pd.concat(global_data, ignore_index=True)\n",
    "    master_df.to_csv(\"VAHAN_DATA_2025.csv\", index=False)\n",
    "    print(\"‚úÖ Saved consolidated data to VAHA_DATA_2023.csv\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data collected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb5e87",
   "metadata": {},
   "source": [
    "AMFII DISTRIBUTOR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress only InsecureRequestWarning\n",
    "warnings.filterwarnings(\"ignore\", category=InsecureRequestWarning)\n",
    "\n",
    "def fetch_distributor_data(output_path, type_, cities, page_size=50):\n",
    "    base_url = \"https://www.amfiindia.com/api/distributor-agent\"\n",
    "    throttle = 2  # seconds between requests\n",
    "    all_records = []\n",
    "\n",
    "    print(f\"Pulling Data for total cities: {len(cities)}.\")\n",
    "\n",
    "    for corp in type_:\n",
    "        for city in cities:\n",
    "            print(f\"Fetching data for city: {city}, type: {corp}...\")\n",
    "            \n",
    "            # First request to get pageCount\n",
    "            params = {\n",
    "                \"strOpt\": corp,\n",
    "                \"city\": city,\n",
    "                \"search\": \"\",\n",
    "                \"page\": 1,\n",
    "                \"pageSize\": page_size\n",
    "            }\n",
    "\n",
    "            response = requests.get(base_url, params=params, verify=False)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            # Collect first page\n",
    "            for rec in data.get(\"data\", []):\n",
    "                rec[\"Type\"] = corp\n",
    "                all_records.append(rec)\n",
    "\n",
    "            # Get total page count\n",
    "            page_count = data.get(\"meta\", {}).get(\"pageCount\", 1)\n",
    "            print(f\"Total pages for {city} ({corp}): {page_count}\")\n",
    "\n",
    "            # Loop through remaining pages\n",
    "            for page in range(2, page_count + 1):\n",
    "                params[\"page\"] = page\n",
    "                response = requests.get(base_url, params=params, verify=False)\n",
    "                response.raise_for_status()\n",
    "                page_data = response.json()\n",
    "                for rec in page_data.get(\"data\", []):\n",
    "                    rec[\"Type\"] = corp\n",
    "                    all_records.append(rec)\n",
    "                time.sleep(throttle)\n",
    "                print(f\"Data fetched for page {page}/{page_count}\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_records)\n",
    "\n",
    "    # Save to single CSV with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = os.path.join(output_path, f\"amfi_dist_{timestamp}.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved {len(df)} rows ‚Üí {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_folder = r\"C:\\Users\\kaustubh.keny\\Downloads\\AMFI_Distributor_Data\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    type_ = [\"Individual\", \"Corporate\"]\n",
    "    \n",
    "    path = r\"cities.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    cities = df.iloc[:,0].to_list()\n",
    "    fetch_distributor_data(output_folder, type_, cities[5250:5500]) # do 5000 to 6000\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e23c0",
   "metadata": {},
   "source": [
    "AMFII TER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_data(config):\n",
    "    response = requests.get(config[\"url\"], params=config[\"payload\"], verify=False)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "   \n",
    "    if \"data\" not in data:\n",
    "        print(\"No 'data' field found in response.\")\n",
    "        return\n",
    "   \n",
    "    records = data[\"data\"]\n",
    "    if not records:\n",
    "        print(\"No records found.\")\n",
    "        return\n",
    "   \n",
    "    headers = list(records[0].keys())\n",
    "   \n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(records)\n",
    "   \n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Fetching data from AMFI API...\")\n",
    "    data = fetch_data(CONFIG)\n",
    "   \n",
    "    time.sleep(CONFIG[\"throttle_seconds\"])\n",
    "\n",
    "    save_to_csv(data, f\"{CONFIG[\"filename\"]}_{CONFIG[\"payload\"][\"date\"]}.csv\")\n",
    "\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "CONFIG = {\n",
    "    \"url\": \"https://www.amfiindia.com/api/tracking-difference\",\n",
    "    \"payload\": {\n",
    "        \"MF_ID\": \"all\",\n",
    "        \"date\": \"01-May-2022\"\n",
    "    },\n",
    "    \"throttle_seconds\": 2,    #delay\n",
    "    \"filename\": \"tracking_error\"\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41755b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_tracking_error(from_date: str, to_date: str, mf_id: str = \"all\", output_file: str = \"tracking_error.xlsx\"):\n",
    "    \"\"\"\n",
    "    Fetch AMFI tracking error data for each day in range and save to one sheet.\n",
    "    \n",
    "    Args:\n",
    "        from_date (str): Start date in format 'dd-mon-yyyy' (e.g. '25-nov-2025')\n",
    "        to_date (str): End date in format 'dd-mon-yyyy'\n",
    "        mf_id (str): Mutual Fund ID parameter (default 'all')\n",
    "        output_file (str): Output Excel file\n",
    "    \"\"\"\n",
    "    # Parse dates\n",
    "    start = datetime.strptime(from_date, \"%d-%b-%Y\")\n",
    "    end = datetime.strptime(to_date, \"%d-%b-%Y\")\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    #day-loop\n",
    "    current = start\n",
    "    while current <= end:\n",
    "        strdt = current.strftime(\"%d-%b-%Y\").lower()  \n",
    "        url = f\"https://www.amfiindia.com/api/tracking-error-data?MF_ID={mf_id}&strdt={strdt}\"\n",
    "        print(f\"Fetching {url} ...\")\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(url, verify=False)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()  # API returns JSON\n",
    "\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            df[\"date\"] = strdt\n",
    "            df[\"MF_ID\"] = mf_id\n",
    "\n",
    "            all_data.append(df)\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for {strdt}: {e}\")\n",
    "\n",
    "        current += timedelta(days=1)\n",
    "\n",
    "   \n",
    "    if all_data:\n",
    "        final_df = pd.concat(all_data, ignore_index=True)\n",
    "        final_df.to_excel(output_file, index=False)\n",
    "        print(f\"Saved {len(final_df)} rows to {output_file}\")\n",
    "    else:\n",
    "        print(\"No data fetched.\")\n",
    "\n",
    "# Example usage\n",
    "fetch_tracking_error(\"01-jun-2022\", \"31-dec-2022\", mf_id=\"all\", output_file=\"tracking_error_22.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9bcf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def fetch_ter_data(output_path, months):\n",
    "    base_url = \"https://www.amfiindia.com/api/populate-te-rdata-revised\"\n",
    "    throttle = 1  # seconds between requests\n",
    "\n",
    "    for month in months:\n",
    "        print(f\"Fetching data for {month}...\")\n",
    "        all_records = []\n",
    "\n",
    "        # First request to get pageCount\n",
    "        params = {\n",
    "            \"MF_ID\": \"All\",\n",
    "            \"Month\": month,\n",
    "            \"strCat\": -1,\n",
    "            \"strType\": -1,\n",
    "            \"page\": 1,\n",
    "            \"pageSize\": 100\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, params=params, verify=False)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        all_records.extend(data.get(\"data\", []))\n",
    "\n",
    "\n",
    "        page_count = data.get(\"meta\", {}).get(\"pageCount\", 1)\n",
    "        print(f\"Total pages for {month}: {page_count}\")\n",
    "\n",
    "        # Loop through remaining pages\n",
    "        for page in range(2, page_count + 1):\n",
    "            params[\"page\"] = page\n",
    "            response = requests.get(base_url, params=params, verify=False)\n",
    "            response.raise_for_status()\n",
    "            page_data = response.json()\n",
    "            all_records.extend(page_data.get(\"data\", []))\n",
    "            time.sleep(throttle)\n",
    "            print(f\"Run completed for {page}/{page_count}.\")\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(all_records)\n",
    "\n",
    "        output_file = os.path.join(output_path, f\"AMFI_TER_{month}.csv\")\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(df)} rows for {month} ‚Üí {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_folder = r\"C:\\Users\\kaustubh.keny\\Downloads\\AMFI_TER_Data\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    months = [\"10-2021\", \"11-2021\", \"12-2021\"] #\"07-2019\", \"08-2019\", \"09-2019\", \"10-2019\", \"11-2019\", \"12-2019\"\n",
    "\n",
    "    fetch_ter_data(output_folder, months)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4563160",
   "metadata": {},
   "source": [
    "AMFII FUND PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abafd09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "url = \"https://www.amfiindia.com/gateway/pollingsebi/api/amfi/fundperformance\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Referer\": \"https://www.amfiindia.com/\"\n",
    "}\n",
    "\n",
    "\n",
    "report_date = \"30-Sep-2025\"  \n",
    "\n",
    "\n",
    "category_map = {\n",
    "    1: [1, 12],\n",
    "    2: [13, 29],\n",
    "    3: [30, 40],\n",
    "    4: [36, 37],\n",
    "    5: [38, 39]\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for maturity_type in [1, 2]:  # Open Ended, Close Ended\n",
    "    for category, subcategories in category_map.items():\n",
    "        for subcat in subcategories:\n",
    "            payload = {\n",
    "                \"maturityType\": maturity_type,\n",
    "                \"category\": category,\n",
    "                \"subCategory\": subcat,\n",
    "                \"mfid\": 0,\n",
    "                \"reportDate\": report_date\n",
    "            }\n",
    "\n",
    "            response = requests.post(url, json=payload, headers=headers, verify=False)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data[\"validationStatus\"] == \"SUCCESS\" and data[\"data\"]:\n",
    "                    df = pd.DataFrame(data[\"data\"])\n",
    "                    df[\"maturityType\"] = maturity_type\n",
    "                    df[\"category\"] = category\n",
    "                    df[\"subCategory\"] = subcat\n",
    "                    results.append(df)\n",
    "                    print(f\"‚úÖ Fetched: M{maturity_type} C{category} S{subcat}\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è No data: M{maturity_type} C{category} S{subcat}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Failed: M{maturity_type} C{category} S{subcat} ‚Üí {response.status_code}\")\n",
    "\n",
    "# Combine all results and save to Excel\n",
    "if results:\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    filename = f\"AMFI_FundPerformance_{report_date.replace('-', '')}.xlsx\"\n",
    "    final_df.to_excel(filename, index=False)\n",
    "    print(f\"\\nüìÅ Data saved to {filename}\")\n",
    "else:\n",
    "    print(\"\\nüö´ No data retrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    " \n",
    "BASE_URL = \"https://api.mospi.gov.in/api/plfs/getData\"\n",
    " \n",
    "def fetch_plfs_all(params: dict, output_file: str = \"plfs_data.csv\", sleep_sec: int = 1):\n",
    " \n",
    "    all_data = []\n",
    " \n",
    "    # Start with page 1 to get metadata\n",
    "    params[\"page\"] = 1\n",
    "    resp = requests.get(BASE_URL, params=params, verify=False)\n",
    "    resp.raise_for_status()\n",
    "    first = resp.json()\n",
    " \n",
    "    if not first.get(\"statusCode\"):\n",
    "        raise Exception(\"API returned error: \" + str(first))\n",
    " \n",
    "    meta = first.get(\"meta_data\", {})\n",
    "    total_pages = meta.get(\"totalPages\", 1)\n",
    "    print(f\"Total pages: {total_pages}\")\n",
    " \n",
    " \n",
    "    if \"data\" in first:\n",
    "        all_data.extend(first[\"data\"])\n",
    " \n",
    "    for page in range(2, total_pages + 1):\n",
    "        params[\"page\"] = page\n",
    "        print(f\"Fetching page {page}/{total_pages} ...\")\n",
    "        try:\n",
    "            resp = requests.get(BASE_URL, params=params)\n",
    "            resp.raise_for_status()\n",
    "            result = resp.json()\n",
    "            if \"data\" in result:\n",
    "                all_data.extend(result[\"data\"])\n",
    "            time.sleep(3)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for page {page}: {e}\")\n",
    "        time.sleep(sleep_sec)\n",
    " \n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(df)} rows to {output_file}\")\n",
    "    else:\n",
    "        print(\"No data fetched.\")\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    payload = {\n",
    "        \"indicator_code\": 8,\n",
    "        \"limit\": 20,\n",
    "        \"year\": \"2017-18,2018-19,2019-20\",\n",
    "        \"age_code\": \"1,2,3,4\",\n",
    "        \"education_code\": \"1,2,3,4,5,6,7,8,9,10\",\n",
    "        \"gender_code\": \"1,2,3\",\n",
    "        \"religion_code\": \"1,2,3,4,5\",\n",
    "        \"sector_code\": \"1,2,3\",\n",
    "        \"social_category_code\": \"1,2,3,4,5\",\n",
    "        \"state_code\": \"1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,99\",\n",
    "        \"weekly_status_code\": \"1,2\",\n",
    "    }\n",
    " \n",
    "    fetch_plfs_all(payload, output_file=f\"Average_Gross_2020-24.csv\", sleep_sec=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
