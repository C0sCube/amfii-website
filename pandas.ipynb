{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1cac2ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[533179, 532466, 543398, 532755, 507685, 532540, 544028, 532400, 532281, 542651]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "csv_path = r\"BSE_CODES.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "codes = df[\"BSE Exchange Code\"].to_list()\n",
    "print(codes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455c3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "url = \"https://api.bseindia.com/BseIndiaAPI/api/AnnSubCategoryGetData/w\"\n",
    "\n",
    "params_template = {\n",
    "    \"pageno\": 1,\n",
    "    \"strCat\": \"Result\",\n",
    "    \"strPrevDate\": \"20250922\",\n",
    "    \"strSearch\": \"P\",\n",
    "    \"strToDate\": \"20251222\",\n",
    "    \"strType\": \"C\",\n",
    "    \"subcategory\": -1\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://www.bseindia.com/\",\n",
    "    \"Origin\": \"https://www.bseindia.com\",\n",
    "    \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "all_dfs = []   # collect DataFrames here\n",
    "\n",
    "for code in codes:\n",
    "    params = params_template.copy()\n",
    "    params[\"strScrip\"] = str(code)\n",
    "\n",
    "    response = session.get(url, params=params, headers=headers, verify=False)\n",
    "    print(\"Scrip\", code, \"-> Status:\", response.status_code)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"Table\" in data and data[\"Table\"]:\n",
    "            df = pd.DataFrame(data[\"Table\"])\n",
    "            # Fix attachment URL\n",
    "            df[\"ATTACHMENTNAME\"] = df[\"ATTACHMENTNAME\"].apply(\n",
    "                lambda x: \"https://www.bseindia.com/xml-data/corpfiling/AttachHis/\" + str(x)\n",
    "            )\n",
    "            # pprint.pprint(df.head(1).to_dict())  # peek at first row\n",
    "            all_dfs.append(df)\n",
    "    else:\n",
    "        print(\"‚ùå Request failed for\", code)\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "# Concatenate all results into one DataFrame\n",
    "if all_dfs:\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    final_df.to_csv(\"all_announcements.csv\", index=False)\n",
    "    print(\"‚úÖ Saved\", len(final_df), \"rows to all_announcements.csv\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b4b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\kaustubh.keny\\Downloads\\reportTable (2).xlsx\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_excel(path)\n",
    "# df.head(20)\n",
    "\n",
    "content = df.iloc[2:,1:]\n",
    "# print(content.shape)\n",
    "category_class = df.columns[0]\n",
    "rto = re.sub(\"\\\\s+\",'',category_class,re.IGNORECASE)\n",
    "\n",
    "content[\"RTO\"] = [rto]*int(content.shape[0])\n",
    "content.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xls_path = r\"C:\\Users\\kaustubh.keny\\Downloads\\allsbe.xls\"\n",
    "xlsx_path = r\"C:\\Users\\kaustubh.keny\\Downloads\\allsbe.xlsx\"\n",
    "\n",
    "xls = pd.ExcelFile(xls_path)\n",
    "with pd.ExcelWriter(xlsx_path, engine=\"openpyxl\") as writer:\n",
    "    for sheet in xls.sheet_names:\n",
    "        print(f\"Converting: {sheet}\")\n",
    "        df = pd.read_excel(xls, sheet_name=sheet)\n",
    "        # Drop completely empty rows/columns to compact\n",
    "        df = df.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "        df.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb5e87",
   "metadata": {},
   "source": [
    "AMFII DISTRIBUTOR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress only InsecureRequestWarning\n",
    "warnings.filterwarnings(\"ignore\", category=InsecureRequestWarning)\n",
    "\n",
    "def fetch_distributor_data(output_path, type_, cities, page_size=50):\n",
    "    base_url = \"https://www.amfiindia.com/api/distributor-agent\"\n",
    "    throttle = 2  # seconds between requests\n",
    "    all_records = []\n",
    "\n",
    "    print(f\"Pulling Data for total cities: {len(cities)}.\")\n",
    "\n",
    "    for corp in type_:\n",
    "        for city in cities:\n",
    "            print(f\"Fetching data for city: {city}, type: {corp}...\")\n",
    "            \n",
    "            # First request to get pageCount\n",
    "            params = {\n",
    "                \"strOpt\": corp,\n",
    "                \"city\": city,\n",
    "                \"search\": \"\",\n",
    "                \"page\": 1,\n",
    "                \"pageSize\": page_size\n",
    "            }\n",
    "\n",
    "            response = requests.get(base_url, params=params, verify=False)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            # Collect first page\n",
    "            for rec in data.get(\"data\", []):\n",
    "                rec[\"Type\"] = corp\n",
    "                all_records.append(rec)\n",
    "\n",
    "            # Get total page count\n",
    "            page_count = data.get(\"meta\", {}).get(\"pageCount\", 1)\n",
    "            print(f\"Total pages for {city} ({corp}): {page_count}\")\n",
    "\n",
    "            # Loop through remaining pages\n",
    "            for page in range(2, page_count + 1):\n",
    "                params[\"page\"] = page\n",
    "                response = requests.get(base_url, params=params, verify=False)\n",
    "                response.raise_for_status()\n",
    "                page_data = response.json()\n",
    "                for rec in page_data.get(\"data\", []):\n",
    "                    rec[\"Type\"] = corp\n",
    "                    all_records.append(rec)\n",
    "                time.sleep(throttle)\n",
    "                print(f\"Data fetched for page {page}/{page_count}\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_records)\n",
    "\n",
    "    # Save to single CSV with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = os.path.join(output_path, f\"amfi_dist_{timestamp}.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved {len(df)} rows ‚Üí {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_folder = r\"C:\\Users\\kaustubh.keny\\Downloads\\AMFI_Distributor_Data\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    type_ = [\"Individual\", \"Corporate\"]\n",
    "    \n",
    "    path = r\"cities.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    cities = df.iloc[:,0].to_list()\n",
    "    fetch_distributor_data(output_folder, type_, cities[5250:5500]) # do 5000 to 6000\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e23c0",
   "metadata": {},
   "source": [
    "AMFII TER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_data(config):\n",
    "    response = requests.get(config[\"url\"], params=config[\"payload\"], verify=False)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "   \n",
    "    if \"data\" not in data:\n",
    "        print(\"No 'data' field found in response.\")\n",
    "        return\n",
    "   \n",
    "    records = data[\"data\"]\n",
    "    if not records:\n",
    "        print(\"No records found.\")\n",
    "        return\n",
    "   \n",
    "    headers = list(records[0].keys())\n",
    "   \n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(records)\n",
    "   \n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Fetching data from AMFI API...\")\n",
    "    data = fetch_data(CONFIG)\n",
    "   \n",
    "    time.sleep(CONFIG[\"throttle_seconds\"])\n",
    "\n",
    "    save_to_csv(data, f\"{CONFIG[\"filename\"]}_{CONFIG[\"payload\"][\"date\"]}.csv\")\n",
    "\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "CONFIG = {\n",
    "    \"url\": \"https://www.amfiindia.com/api/tracking-difference\",\n",
    "    \"payload\": {\n",
    "        \"MF_ID\": \"all\",\n",
    "        \"date\": \"01-May-2022\"\n",
    "    },\n",
    "    \"throttle_seconds\": 2,    #delay\n",
    "    \"filename\": \"tracking_error\"\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41755b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_tracking_error(from_date: str, to_date: str, mf_id: str = \"all\", output_file: str = \"tracking_error.xlsx\"):\n",
    "    \"\"\"\n",
    "    Fetch AMFI tracking error data for each day in range and save to one sheet.\n",
    "    \n",
    "    Args:\n",
    "        from_date (str): Start date in format 'dd-mon-yyyy' (e.g. '25-nov-2025')\n",
    "        to_date (str): End date in format 'dd-mon-yyyy'\n",
    "        mf_id (str): Mutual Fund ID parameter (default 'all')\n",
    "        output_file (str): Output Excel file\n",
    "    \"\"\"\n",
    "    # Parse dates\n",
    "    start = datetime.strptime(from_date, \"%d-%b-%Y\")\n",
    "    end = datetime.strptime(to_date, \"%d-%b-%Y\")\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    #day-loop\n",
    "    current = start\n",
    "    while current <= end:\n",
    "        strdt = current.strftime(\"%d-%b-%Y\").lower()  \n",
    "        url = f\"https://www.amfiindia.com/api/tracking-error-data?MF_ID={mf_id}&strdt={strdt}\"\n",
    "        print(f\"Fetching {url} ...\")\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(url, verify=False)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()  # API returns JSON\n",
    "\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            df[\"date\"] = strdt\n",
    "            df[\"MF_ID\"] = mf_id\n",
    "\n",
    "            all_data.append(df)\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for {strdt}: {e}\")\n",
    "\n",
    "        current += timedelta(days=1)\n",
    "\n",
    "   \n",
    "    if all_data:\n",
    "        final_df = pd.concat(all_data, ignore_index=True)\n",
    "        final_df.to_excel(output_file, index=False)\n",
    "        print(f\"Saved {len(final_df)} rows to {output_file}\")\n",
    "    else:\n",
    "        print(\"No data fetched.\")\n",
    "\n",
    "# Example usage\n",
    "fetch_tracking_error(\"01-jun-2022\", \"31-dec-2022\", mf_id=\"all\", output_file=\"tracking_error_22.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9bcf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def fetch_ter_data(output_path, months):\n",
    "    base_url = \"https://www.amfiindia.com/api/populate-te-rdata-revised\"\n",
    "    throttle = 1  # seconds between requests\n",
    "\n",
    "    for month in months:\n",
    "        print(f\"Fetching data for {month}...\")\n",
    "        all_records = []\n",
    "\n",
    "        # First request to get pageCount\n",
    "        params = {\n",
    "            \"MF_ID\": \"All\",\n",
    "            \"Month\": month,\n",
    "            \"strCat\": -1,\n",
    "            \"strType\": -1,\n",
    "            \"page\": 1,\n",
    "            \"pageSize\": 100\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, params=params, verify=False)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        all_records.extend(data.get(\"data\", []))\n",
    "\n",
    "\n",
    "        page_count = data.get(\"meta\", {}).get(\"pageCount\", 1)\n",
    "        print(f\"Total pages for {month}: {page_count}\")\n",
    "\n",
    "        # Loop through remaining pages\n",
    "        for page in range(2, page_count + 1):\n",
    "            params[\"page\"] = page\n",
    "            response = requests.get(base_url, params=params, verify=False)\n",
    "            response.raise_for_status()\n",
    "            page_data = response.json()\n",
    "            all_records.extend(page_data.get(\"data\", []))\n",
    "            time.sleep(throttle)\n",
    "            print(f\"Run completed for {page}/{page_count}.\")\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(all_records)\n",
    "\n",
    "        output_file = os.path.join(output_path, f\"AMFI_TER_{month}.csv\")\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(df)} rows for {month} ‚Üí {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_folder = r\"C:\\Users\\kaustubh.keny\\Downloads\\AMFI_TER_Data\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    months = [\"10-2021\", \"11-2021\", \"12-2021\"] #\"07-2019\", \"08-2019\", \"09-2019\", \"10-2019\", \"11-2019\", \"12-2019\"\n",
    "\n",
    "    fetch_ter_data(output_folder, months)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4563160",
   "metadata": {},
   "source": [
    "AMFII FUND PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abafd09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "url = \"https://www.amfiindia.com/gateway/pollingsebi/api/amfi/fundperformance\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Referer\": \"https://www.amfiindia.com/\"\n",
    "}\n",
    "\n",
    "\n",
    "report_date = \"30-Sep-2025\"  \n",
    "\n",
    "\n",
    "category_map = {\n",
    "    1: [1, 12],\n",
    "    2: [13, 29],\n",
    "    3: [30, 40],\n",
    "    4: [36, 37],\n",
    "    5: [38, 39]\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for maturity_type in [1, 2]:  # Open Ended, Close Ended\n",
    "    for category, subcategories in category_map.items():\n",
    "        for subcat in subcategories:\n",
    "            payload = {\n",
    "                \"maturityType\": maturity_type,\n",
    "                \"category\": category,\n",
    "                \"subCategory\": subcat,\n",
    "                \"mfid\": 0,\n",
    "                \"reportDate\": report_date\n",
    "            }\n",
    "\n",
    "            response = requests.post(url, json=payload, headers=headers, verify=False)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data[\"validationStatus\"] == \"SUCCESS\" and data[\"data\"]:\n",
    "                    df = pd.DataFrame(data[\"data\"])\n",
    "                    df[\"maturityType\"] = maturity_type\n",
    "                    df[\"category\"] = category\n",
    "                    df[\"subCategory\"] = subcat\n",
    "                    results.append(df)\n",
    "                    print(f\"‚úÖ Fetched: M{maturity_type} C{category} S{subcat}\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è No data: M{maturity_type} C{category} S{subcat}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Failed: M{maturity_type} C{category} S{subcat} ‚Üí {response.status_code}\")\n",
    "\n",
    "# Combine all results and save to Excel\n",
    "if results:\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    filename = f\"AMFI_FundPerformance_{report_date.replace('-', '')}.xlsx\"\n",
    "    final_df.to_excel(filename, index=False)\n",
    "    print(f\"\\nüìÅ Data saved to {filename}\")\n",
    "else:\n",
    "    print(\"\\nüö´ No data retrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    " \n",
    "BASE_URL = \"https://api.mospi.gov.in/api/plfs/getData\"\n",
    " \n",
    "def fetch_plfs_all(params: dict, output_file: str = \"plfs_data.csv\", sleep_sec: int = 1):\n",
    " \n",
    "    all_data = []\n",
    " \n",
    "    # Start with page 1 to get metadata\n",
    "    params[\"page\"] = 1\n",
    "    resp = requests.get(BASE_URL, params=params, verify=False)\n",
    "    resp.raise_for_status()\n",
    "    first = resp.json()\n",
    " \n",
    "    if not first.get(\"statusCode\"):\n",
    "        raise Exception(\"API returned error: \" + str(first))\n",
    " \n",
    "    meta = first.get(\"meta_data\", {})\n",
    "    total_pages = meta.get(\"totalPages\", 1)\n",
    "    print(f\"Total pages: {total_pages}\")\n",
    " \n",
    " \n",
    "    if \"data\" in first:\n",
    "        all_data.extend(first[\"data\"])\n",
    " \n",
    "    for page in range(2, total_pages + 1):\n",
    "        params[\"page\"] = page\n",
    "        print(f\"Fetching page {page}/{total_pages} ...\")\n",
    "        try:\n",
    "            resp = requests.get(BASE_URL, params=params)\n",
    "            resp.raise_for_status()\n",
    "            result = resp.json()\n",
    "            if \"data\" in result:\n",
    "                all_data.extend(result[\"data\"])\n",
    "            time.sleep(3)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for page {page}: {e}\")\n",
    "        time.sleep(sleep_sec)\n",
    " \n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(df)} rows to {output_file}\")\n",
    "    else:\n",
    "        print(\"No data fetched.\")\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    payload = {\n",
    "        \"indicator_code\": 8,\n",
    "        \"limit\": 20,\n",
    "        \"year\": \"2017-18,2018-19,2019-20\",\n",
    "        \"age_code\": \"1,2,3,4\",\n",
    "        \"education_code\": \"1,2,3,4,5,6,7,8,9,10\",\n",
    "        \"gender_code\": \"1,2,3\",\n",
    "        \"religion_code\": \"1,2,3,4,5\",\n",
    "        \"sector_code\": \"1,2,3\",\n",
    "        \"social_category_code\": \"1,2,3,4,5\",\n",
    "        \"state_code\": \"1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,99\",\n",
    "        \"weekly_status_code\": \"1,2\",\n",
    "    }\n",
    " \n",
    "    fetch_plfs_all(payload, output_file=f\"Average_Gross_2020-24.csv\", sleep_sec=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
